{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielutomo/Youtube-Music-Hits/blob/main/Naive%20Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORT LIBRARY"
      ],
      "metadata": {
        "id": "Bd1yGyOYXiQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WEdnfHmvWNzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MEMBACA DATASET"
      ],
      "metadata": {
        "id": "Jz8e4zotX6T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/youtube-top-100-songs-2025 (3).csv\")\n",
        "\n",
        "# Lihat 5 data pertama\n",
        "display(data.head())\n",
        "\n",
        "# Cek struktur data\n",
        "data.info()\n"
      ],
      "metadata": {
        "id": "d1kYK4D2XRXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "286c6748"
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/youtube-top-100-songs-2025 (3).csv\")\n",
        "\n",
        "# Lihat 5 data pertama\n",
        "display(data.head())\n",
        "\n",
        "# Cek struktur data melihat struktur dataset, seperti title, categories, dan apakah ada data kosong.”\n",
        "data.info()\n",
        "# Dari 100 data, hanya 85 video punya tags,15 video tidak memiliki tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CEK APAKAH ADA DATA YANG NULL DAN DUPLIKAT"
      ],
      "metadata": {
        "id": "bE_xF419YATA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cek data kosong (null)\n",
        "print(\"=== CEK DATA NULL ===\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Cek duplikat\n",
        "print(\"\\nJumlah duplikat:\", data.duplicated().sum())\n",
        "\n",
        "# Hapus duplikat jika ada\n",
        "data = data.drop_duplicates()\n",
        "# mengecek apakah dataset memiliki data kosong pada setiap kolom."
      ],
      "metadata": {
        "id": "skLrVPxsXT_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MEMISAHKAN FITUR DAN LABEL"
      ],
      "metadata": {
        "id": "49taqme5YNtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# X = variabel input (fitur) yang digunakan untuk memprediksi sesuatu\n",
        "# y = label atau target yang ingin diprediksi\n",
        "# Tentukan fitur dan label\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Copy data agar aman\n",
        "df = data.copy()\n",
        "\n",
        "# Buat encoder\n",
        "le_categories = LabelEncoder()\n",
        "le_tags = LabelEncoder()\n",
        "le_channel = LabelEncoder()\n",
        "\n",
        "# Encode fitur kategorikal\n",
        "df['categories_encoded'] = le_categories.fit_transform(df['categories'])\n",
        "df['tags_encoded'] = le_tags.fit_transform(df['tags'].astype(str))  # pastikan string\n",
        "df['channel_encoded'] = le_channel.fit_transform(df['channel'])\n",
        "\n",
        "# Tentukan fitur dan label\n",
        "X = df[['categories_encoded', 'duration', 'tags_encoded', 'channel_encoded']]\n",
        "y = df['view_count']  # asli, tidak di-encode\n",
        "\n",
        "print(\"Contoh fitur:\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"\\nContoh label (view_count asli):\")\n",
        "print(y.head())\n",
        "# Ini adalah data input (X) yang akan digunakan model untuk belajar memprediksi view_count."
      ],
      "metadata": {
        "id": "oaqDq6RWXVa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BALANCING DATA MENGGUNAKAN SMOTE"
      ],
      "metadata": {
        "id": "Fjwr6Rx0YS_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Cek distribusi sebelum SMOTE\n",
        "# LabelEncoder Mengubah view_count menjadi kelas karena nanti dipakai SMOTE (SMOTE hanya bisa untuk classification)\n",
        "print(\"=== Distribusi Sebelum Balancing ===\")\n",
        "print(pd.Series(y_encoded).value_counts())\n",
        "\n",
        "# Handle missing values in 'tags' (This line caused the KeyError and is removed)\n",
        "# X['tags'].fillna('', inplace=True)\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_features = ['categories', 'tags', 'channel']\n",
        "one_hot = OneHotEncoder(handle_unknown='ignore')\n",
        "transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)],\n",
        "                                  remainder='passthrough')\n",
        "\n",
        "# For X_encoded, we need to use the original df for OneHotEncoder, not X (which is already label encoded)\n",
        "# The X passed to transformer.fit_transform should contain the raw categorical features for OneHotEncoder.\n",
        "# Let's reconstruct X for this step, based on df, before applying OneHotEncoder.\n",
        "\n",
        "# Recreate X with original categorical columns for OneHotEncoder\n",
        "X_for_onehot = df[['categories', 'duration', 'tags', 'channel']].copy()\n",
        "# Fill NA in tags for X_for_onehot before OneHotEncoding if desired\n",
        "X_for_onehot['tags'].fillna('', inplace=True)\n",
        "\n",
        "X_encoded = transformer.fit_transform(X_for_onehot)\n",
        "\n",
        "\n",
        "# Terapkan SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_encoded, y_encoded)\n",
        "\n",
        "# Cek distribusi setelah SMOTE (Membuat kelas menjadi seimbang dengan menambah data sintetis)\n",
        "print(\"\\n=== Distribusi Setelah SMOTE ===\")\n",
        "print(pd.Series(y_resampled).value_counts())\n",
        "\n",
        "# SMOTE tidak mengubah apa pun.\n",
        "# Distribusi label tetap sama persis, jumlahnya tetap 100 kelas dengan frekuensi 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "IJEfGt_3XYHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NORMALISASI DAN SPLIT DATA"
      ],
      "metadata": {
        "id": "Xr1UfHgWYYBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalisasi fitur agar skala serupa\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_scaled = scaler.fit_transform(X_resampled)\n",
        "\n",
        "# Bagi data menjadi train dan test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_resampled, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Data training:\", X_train.shape)\n",
        "print(\"Data testing:\", X_test.shape)\n",
        "\n",
        "# membagi data menjadi data latih (train) dan data uji (test)"
      ],
      "metadata": {
        "id": "iCfd2ZuSXZyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING MODEL NAIVE BAYES"
      ],
      "metadata": {
        "id": "2ArIRuyiYbbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Buat model Naive Bayes Gaussian Digunakan untuk memprediksi berdasarkan distribusi Gaussian (normal).\n",
        "model = GaussianNB()\n",
        "\n",
        "# Latih model\n",
        "# Convert sparse matrix to dense array\n",
        "model.fit(X_train.toarray(), y_train)\n",
        "\n",
        "print(\"Model berhasil dilatih ✅\")"
      ],
      "metadata": {
        "id": "5I4xO-BcXbP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EVALUASI MODEL"
      ],
      "metadata": {
        "id": "NEO6plVVYf-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Import numpy for unique\n",
        "\n",
        "# Prediksi data uji Model Naive Bayes yang sudah dilatih digunakan untuk memprediksi label dari data test.\n",
        "y_pred = model.predict(X_test.toarray())\n",
        "\n",
        "# Menggabungkan semua label asli (y_test) dan label prediksi (y_pred).\n",
        "unique_labels = np.unique(np.concatenate((y_test, y_pred)))\n",
        "\n",
        "# Di bagian ini, label angka (hasil encoding) dikembalikan ke nilai view_count asli, supaya confusion matrix labelnya terbaca (angka view_count asli)\n",
        "target_names_subset = le.inverse_transform(unique_labels).astype(str).tolist()\n",
        "\n",
        "\n",
        "# Evaluasi hasil Menghitung persentase prediksi yang benar.\n",
        "print(\"=== HASIL EVALUASI ===\")\n",
        "print(f\"Akurasi: {accuracy_score(y_test, y_pred)*100:.2f}%\\n\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, labels=unique_labels, target_names=target_names_subset, zero_division=0))\n",
        "\n",
        "# Confusion Matrix visual\n",
        "cm = confusion_matrix(y_test, y_pred, labels=unique_labels)\n",
        "plt.figure(figsize=(8, 6)) # Increase figure size for better readability\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names_subset, yticklabels=target_names_subset)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kUSn-e7nXcyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PREDIKSI MANUAL (INPUT DARI USER)"
      ],
      "metadata": {
        "id": "VQmwQqDlYiR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Input manual - ask for the correct features and handle types\n",
        "categories_input = input(\"Masukkan kategori (contoh: Music): \")\n",
        "duration_input = float(input(\"Masukkan durasi dalam detik (contoh: 173): \"))\n",
        "tags_input = input(\"Masukkan tags (pisahkan dengan semicolon, contoh: Pop;Rock): \")\n",
        "channel_input = input(\"Masukkan nama channel (contoh: ROSÉ): \")\n",
        "\n",
        "\n",
        "# Create a DataFrame with the same column names as the training data\n",
        "input_df = pd.DataFrame([[categories_input, duration_input, tags_input, channel_input]],\n",
        "                        columns=['categories', 'duration', 'tags', 'channel'])\n",
        "\n",
        "# Apply the same preprocessing as used for the training data\n",
        "# We need to use the fitted preprocessor and scaler\n",
        "input_processed = preprocessor.transform(input_df)\n",
        "input_scaled = scaler.transform(input_processed)\n",
        "\n",
        "\n",
        "# Prediksi\n",
        "pred = model.predict(input_scaled.toarray()) # Convert to dense array for GaussianNB\n",
        "pred_label = le.inverse_transform(pred)\n",
        "\n",
        "print(f\"\\nPrediksi View Count: {pred_label[0]}\")"
      ],
      "metadata": {
        "id": "K05hVhvHsx-N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}